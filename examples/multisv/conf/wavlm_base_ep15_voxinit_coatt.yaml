### train configuraton
PORT: 25182
exp_dir: exp/wavlm_base_ep15_voxinit_coatt
gpus: "[0,1,2,3,4,5,6,7]"
num_avg: 4
enable_amp: False # whether enable automatic mixed precision training

seed: 42
num_epochs: 15
save_epoch_interval: 4 # save model every 4 epochs
log_batch_interval: 100 # log every 100 batchs

spkid2label_map_path: "data/spkid2label.txt"

dataloader_args:
  batch_size: 32
  num_workers: 2
  pin_memory: True
  prefetch_factor: 2
  drop_last: True

dataset_args:
  # the sample number which will be traversed within one epoch, if the value equals to 0,
  # the utterance number in the dataset will be used as the sample_num_per_epoch.
  sample_num_per_epoch: 0
  shuffle: True
  shuffle_args:
    shuffle_size: 2500
  filter: True
  filter_args:
    min_num_frames: 250
    max_num_frames: 800
  resample_rate: 16000
  speed_perturb: False
  num_frms: 300
  aug_prob: 0.0 # prob to add reverb & noise aug per sample
  raw_wav: True
  fbank_args:
    num_mel_bins: 80
    frame_shift: 10
    frame_length: 25
    dither: 1.0
  spec_aug: False
  spec_aug_args:
    num_t_mask: 1
    num_f_mask: 1
    max_t: 10
    max_f: 8
    prob: 0.6

model: WavLM_Base_MC_MHFA_no_gradmult
model_init: null
model_args:
  model_path: "data/sc_vox_wavlm_mhfa/sc_vox_trained.pt"
  head_nb: 64
  embed_dim: 256
  pooling: "MHFA" #Group 
  group: 1
  cnn_scale: 0.0
  n_chans: 4
  fusion_modules: "CoAttention:single_dim=(int)128;multi_dim=(int)32,CoAttention:single_dim=(int)128;multi_dim=(int)32,CoAttention:single_dim=(int)128;multi_dim=(int)32,CoAttention:single_dim=(int)128;multi_dim=(int)32,MicWeightedAvg:ref_mic_w=(float)0.25"
  cnn_out_fusion: "CoAttention:single_dim=(int)128;multi_dim=(int)32"
  rep_fusion: "wavg:0.25"

projection_args:
  project_type: "arc_margin" # add_margin, arc_margin, arc_margin_intertopk_subcenter, sphere, softmax
  scale: 30.0
  easy_margin: False

margin_scheduler: MarginScheduler
margin_update:
  initial_margin: 0.0
  final_margin: 0.2
  increase_start_epoch: 2
  fix_start_epoch: 5
  update_margin: True
  increase_type: "exp" # exp, linear

loss: CrossEntropyLoss
loss_args: {}

optimizer: AdamW
optimizer_args:
  weight_decay: 0.01 # AdamW default

scheduler: ExponentialDecrease
scheduler_args:
  initial_lr: 1.0e-4
  final_lr: 4.4e-5
  warm_up_epoch: 3
  warm_from_zero: True
  lr_scaling: [1.0, 0.2]
